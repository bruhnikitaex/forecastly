# src/models/predict.py
"""
–ú–æ–¥—É–ª—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ Prophet –∏ XGBoost –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤.
–ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –æ–±—É—á–∞–µ—Ç –∏—Ö –Ω–∞ –ª–µ—Ç—É.
"""
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
import joblib
from prophet import Prophet

from src.utils.logger import logger
from src.utils.config import PATHS
from src.etl.load_data import load_sales
from src.etl.clean_data import clean_sales
from src.etl.feature_builder import build_features


def normalize_sku(s) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º –í–°–Å –∫ –≤–∏–¥—É SKUxxx (–±–µ–∑ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è),
    –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ –¥–∞—à–±–æ—Ä–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç.
    """
    s = str(s).strip().upper()
    # —É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    s = s.replace("SKU_", "SKU")
    s = s.replace("SKU-", "SKU")
    if s.startswith("SKU"):
        num = s[3:]
        num = "".join(ch for ch in num if ch.isdigit())
        return f"SKU{num.zfill(3)}"
    # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç
    digits = "".join(ch for ch in s if ch.isdigit())
    if digits:
        return f"SKU{digits.zfill(3)}"
    return s


def load_prophet_models() -> dict:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ Prophet.

    Returns:
        –°–ª–æ–≤–∞—Ä—å {sku_id: Prophet_model} –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å.
    """
    model_path = Path(PATHS['data']['models_dir']) / 'prophet_model.pkl'
    if model_path.exists():
        try:
            models = joblib.load(model_path)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models)} –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Prophet –º–æ–¥–µ–ª–µ–π")
            return models
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Prophet –º–æ–¥–µ–ª–∏: {e}")
    return {}


def predict(horizon: int = 14):
    logger.info(f"Start predicting, horizon={horizon}")

    # 1. –∑–∞–≥—Ä—É–∑–∏–ª–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ
    df = load_sales()
    df = clean_sales(df)
    df = build_features(df)
    df = df.sort_values(['sku_id', 'store_id', 'date'])

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    prophet_models = load_prophet_models()

    results = []

    # 2. –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –∫–∞–∂–¥–æ–º—É SKU
    for sku, g in df.groupby('sku_id'):
        sku_norm = normalize_sku(sku)
        g = g.sort_values('date')
        last_date = g['date'].max()

        # -------- Prophet --------
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            if sku in prophet_models:
                m = prophet_models[sku]
                logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Prophet –¥–ª—è {sku_norm}")
            elif sku_norm in prophet_models:
                m = prophet_models[sku_norm]
                logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Prophet –¥–ª—è {sku_norm}")
            else:
                # –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π Prophet –º–æ–¥–µ–ª–∏ –¥–ª—è {sku_norm}")
                m = Prophet()
                df_p = g[['date', 'units']].rename(columns={'date': 'ds', 'units': 'y'})
                m.fit(df_p)

            fut = pd.DataFrame({'ds': [last_date + pd.Timedelta(days=i) for i in range(1, horizon + 1)]})
            fc = m.predict(fut)
            prophet_pred = fc[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
            prophet_pred.columns = ['date', 'prophet', 'p_low', 'p_high']
        except Exception as e:
            logger.warning(f"Prophet failed on {sku_norm}: {e}")
            prophet_pred = pd.DataFrame(columns=['date', 'prophet', 'p_low', 'p_high'])

        # -------- XGBoost --------
        try:
            model_path = Path(PATHS['data']['models_dir']) / 'xgboost_model.pkl'
            model = joblib.load(model_path)

            g_feat = build_features(g.copy())
            last_rows = g_feat.tail(60).copy()
            X_last = last_rows[['dow', 'week', 'month', 'units_lag_1', 'units_lag_7']].fillna(0)

            preds = []
            cur = X_last.iloc[-1:].copy()
            for i in range(horizon):
                p = model.predict(cur)[0]
                preds.append(float(p))
                # –æ–±–Ω–æ–≤–∏–º –ª–∞–≥–∏
                cur['units_lag_1'] = p
                cur['units_lag_7'] = preds[i - 6] if i >= 6 else p

            xgb_pred = pd.DataFrame({
                'date': [last_date + pd.Timedelta(days=i + 1) for i in range(horizon)],
                'xgb': preds
            })
        except Exception as e:
            logger.warning(f"XGBoost failed on {sku_norm}: {e}")
            xgb_pred = pd.DataFrame(columns=['date', 'xgb'])

        # -------- Ensemble --------
        df_merge = pd.merge(prophet_pred, xgb_pred, on='date', how='outer')
        df_merge['ensemble'] = df_merge[['prophet', 'xgb']].mean(axis=1)
        df_merge['sku_id'] = sku_norm
        results.append(df_merge)

    # 3. —Å–∫–ª–µ–∏–ª–∏ –≤—Å—ë
    all_pred = pd.concat(results, ignore_index=True)

    # 4. –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å (–µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –±—ã–ª –∫—Ä–∏–≤–æ–π)
    base = Path(PATHS['data']['processed'])
    # –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ —É–∫–∞–∑–∞–Ω–æ .../processed.parquet ‚Äî –±–µ—Ä—ë–º —Ä–æ–¥–∏—Ç–µ–ª—è
    if base.suffix:
        base = base.parent
    if base.exists() and not base.is_dir():
        base.unlink()
    base.mkdir(parents=True, exist_ok=True)

    # –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç –¥–∞—à–±–æ—Ä–¥
    out_path = base / 'predictions.csv'
    all_pred.to_csv(out_path, index=False)

    # 5. —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä—Å–∏—é –≤ –∏—Å—Ç–æ—Ä–∏—é
    hist_dir = base / "history"
    hist_dir.mkdir(parents=True, exist_ok=True)
    ts = pd.Timestamp.utcnow().strftime("%Y-%m-%d_%H%M%S")
    hist_path = hist_dir / f"predictions_{ts}.csv"
    all_pred.to_csv(hist_path, index=False)

    logger.info(f"‚úÖ Predictions saved to {out_path}")
    logger.info(f"üóÇ  History saved to {hist_path}")
    print(f"‚úÖ Predictions saved to {out_path}")
    print(f"üóÇ  History saved to {hist_path}")
    return out_path


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--horizon", type=int, default=14)
    args = ap.parse_args()
    predict(horizon=args.horizon)
